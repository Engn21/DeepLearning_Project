{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlO-WIRxbAC2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "# We are downloading and then reading the data.\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).text\n",
        "\n",
        "# The dataset class which is inherited from Dataset Class of PyTorch\n",
        "class CharDataset(Dataset):\n",
        "    #This is the constructer function:\n",
        "    #data: the entire Shakespeare text.\n",
        "    #block_size: the maximum context length the model will see.\n",
        "    #It will not see the whole context at once\n",
        "    #only 128 characters at once for this case\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "        chars = sorted(list(set(data)))\n",
        "        # Here we are finding the unique characters- Vocabulary\n",
        "        #set(data) → takes all unique characters\n",
        "        #list(...) → converts the set to a list\n",
        "        #sorted(...) → sorts alphabetically\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        # Character -> Number and,  Number -> Character conversion dictionaries\n",
        "        #We do this tokenization so the neural network understands the character\n",
        "        #As long as we work with the same data, the character-index mapping always remains the same.\n",
        "        #It's because from chars, the lists comes ordered\n",
        "        self.vocab_size = len(chars)\n",
        "        #for this case our vocab size is 65\n",
        "        #This information is required for the model's embedding table, output layer, etc.\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "        # Returns how many samples we can extract from the dataset\n",
        "        #We extract the block size from the lenght of the data\n",
        "        # so in the last block we don't have empty characters\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Take a piece of text that is block_size + 1 characters long\n",
        "        #We add the plus 1 since the last index is not included in the a:b form\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # Convert characters to numbers\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # Return the chunk and the shifted version as tensors\n",
        "        # x: The input sequence you will provide to the model (character IDs)(0 to N-1)\n",
        "        # y: The target sequence you want the model to predict (the next character IDs)(1 to N)\n",
        "        #This is the next token prediction concept\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "# Note: Since I was using Google Colab, I reduced the Batch Size to ensure sufficient RAM\n",
        "batch_size = 64      # B: Reduced to manage RAM usage with the larger model\n",
        "block_size = 128     # N: Context window size\n",
        "#Batch size: 32 means the code updates the model after reading 32 pieces of text.\n",
        "#Each of these 32 pieces has 128 characters\n",
        "max_iters = 6000     # Total training steps\n",
        "#This updating process will continue 6000 times\n",
        "#I tried with 3000 but the result wasn't as good\n",
        "#And I had a final loss value as 2.0769\n",
        "learning_rate = 3e-4 # Lower learning rate for stability with deep model\n",
        "#I first used 3e-4 but the loss didn't decrease enough with this value\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(f\"Kullanılan Cihaz: {device}\")\n",
        "n_embd = 768       #This means the number of embeddings, each character is presented by 768 attributes\n",
        "n_head = 8\n",
        "n_layer = 12       # Number of layers is 12, showing the depth of the code\n",
        "dropout = 0.1      # Standart. This means that we randomly deactivate 10% of neurons during training.\n",
        "                  #This is done to prevent Over-Fitting.\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "class CausalSelfAttn(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "        # Key, Query, Value projection\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        #When attention scores are calculated (after Softmax),\n",
        "        #it randomly drops some connections. This prevents the model from memorizing\n",
        "        #that, this word is 100% connected to that\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Causal Mask (So the model doesn't see the future)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
        "                                     .view(1, 1, block_size, block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "         # Split into heads (B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "\n",
        "        # Attention Scors (Scaled Dot-Product)\n",
        "        # Causal Self-Attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1)**0.5))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "\n",
        "        y = att @ v # (B, n_head, T, head_size)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs side by side\n",
        "\n",
        "        return self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  #Multi-Layer Perceptron (Feed-Forward Network)\n",
        "  #The tokens starts processing the information after the Attention procedure. Attention procedure stands for\n",
        "    def __init__(self, n_embd):\n",
        "        #Initialize the MLP with two linear layers and a GELU activation function\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            #here we are increasing the dimensionality from n_embd to 4*n_embd and then reducing it back to n_embd because this is\n",
        "            #a common practice in transformer architectures to allow the model to learn more complex representations\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            #The GELU activation function introduces non-linearity, enabling the model to capture complex patterns in the data.\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "            #Dropout is applied after the second linear layer to prevent overfitting by randomly deactivating some neurons during training.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"The forward method defines how the input data flows through the MLP network.\"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Here we are defining a Transformer Block that combines Causal Self-Attention and MLP with Layer Normalization and Residual Connections.\"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # Initialize the Block with LayerNorm, CausalSelfAttn, and MLP to process the input data.\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttn(n_embd, n_head)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = MLP(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Here x = x + self.CausalSelfAttn(self.LayerNorm_1(x)) implementation\n",
        "        # Residual connection around the attention layer\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        # out = x + self.MLP(self.LayerNorm_2(x)) which is another residual connection around the MLP layer\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    \"\"\"Here we are defining the GPT Language Model that combines token and position embeddings,\n",
        "      multiple Transformer Blocks, and a final output layer for language modeling.\"\"\"\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, n_embd)  # Token embeddings\n",
        "        self.wpe = nn.Embedding(block_size, n_embd)  # Position embeddings\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # Final LayerNorm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.wte.weight = self.lm_head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # tok_emb = WTE(idx)\n",
        "        tok_emb = self.wte(idx)\n",
        "\n",
        "        # pos_emb = WPE(pos)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
        "        pos_emb = self.wpe(pos)\n",
        "\n",
        "        # x = Dropout(tok_emb + pos_emb)\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # for Block in Blocks: x = Block(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # x = Final_LayerNorm(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # logits = LM_Head(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \"\"\"Here this function generates new tokens given a context so that the model can produce text.\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            #this loop will run for the number of tokens we want to generate\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            #the idx_cond variable validates that the context length does not exceed block_size\n",
        "            #idx[:, -block_size:] means taking the last 'block_size' tokens from idx\n",
        "            #So if idx has more than block_size tokens, we only consider the most recent ones\n",
        "            logits, _ = self(idx_cond)\n",
        "            #logits, _ means we are only interested in the logits output\n",
        "            #The logits represent the model's raw predictions for the next token probabilities\n",
        "            #We are equalizing the context to self(idx_cond) to get the logits for the next token prediction.\n",
        "            # We are doing this because the model generates the next token based on the provided context.\n",
        "            logits = logits[:, -1, :]\n",
        "            #Here, we are extracting the logits for the last time step\n",
        "            #logits[:, -1, :] means taking all batches (:), the last time step (-1), and all classes (:)\n",
        "            #This gives us the model's predictions for the next token based on the entire context\n", 
        "            probs = F.softmax(logits , dim=-1)\n",
        "\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "#The training begins here\n",
        "\n",
        "# Here below we are taking the whole sheakspeare text and length with block_size, and using it to train the model with\n",
        "# input-output pairs dataset that has been iterated one character creation\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "# Also below getting ready the dataset to be trained. Also in each step the code gives samples from the dataset based on the batch size\n",
        "#by using shuffle=True, we ensure that the data is shuffled before each epoch that prevents the model from learning the order of the data\n",
        "#This is important for generalization. num_workers=0 means that the data loading will be done in the main process.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# Model Starts\n",
        "model = GPTLanguageModel(vocab_size=train_dataset.get_vocab_size())\n",
        "#here above model is equalized to the G\n",
        "m = model.to(device)\n",
        "print(f\"Count of mode: {sum(p.numel() for p in m.parameters())/1e6:.2f} Milyon\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Training Starts\\n\")\n",
        "model.train()\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "for iter_num in range(max_iters):\n",
        "    try:\n",
        "        xb, yb = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(train_loader)\n",
        "        xb, yb = next(data_iter)\n",
        "\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iter_num % 100 == 0:\n",
        "        print(f\"Step {iter_num}: Loss {loss.item():.4f}\")\n",
        "        #Prints the each 100 step and the current loss\n",
        "\n",
        "print(f\"Training Finished. Final Loss: {loss.item():.4f}\")\n",
        "\n",
        "#Evaluation and inference happens here\n",
        "\n",
        "print(\"\\n--- Inference ---\")\n",
        "\n",
        "# Tokenizer functions\n",
        "def tokenize(s):\n",
        "    return torch.tensor([train_dataset.stoi[c] for c in s], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    return ''.join([train_dataset.itos[i.item()] for i in tokens[0]])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Starter context\n",
        "    context_str = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context_str)\n",
        "\n",
        "    # Generate\n",
        "    y = model.generate(tokenized_context, max_new_tokens=500)\n",
        "\n",
        "    # Decode\n",
        "    completion = tokens_to_string(y)\n",
        "\n",
        "    print(completion)"
      ]
    }
  ]
}
