{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTAvVR2TXHyS",
        "outputId": "80ebb6e0-9085-41df-8e97-dd99e78ccae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cihaz: cuda\n",
            "Model parametre sayısı: 85.20 Milyon\n",
            "Eğitim başlıyor (Büyük model olduğu için biraz daha yavaş olabilir)...\n",
            "Adım 0: Loss 4.3788\n",
            "Adım 100: Loss 3.3377\n",
            "Adım 200: Loss 3.3308\n",
            "Adım 300: Loss 3.2818\n",
            "Adım 400: Loss 3.3214\n",
            "Adım 500: Loss 3.3046\n",
            "Adım 600: Loss 3.3011\n",
            "Adım 700: Loss 3.2146\n",
            "Adım 800: Loss 3.1510\n",
            "Adım 900: Loss 3.0241\n",
            "Adım 1000: Loss 2.9012\n",
            "Adım 1100: Loss 2.7980\n",
            "Adım 1200: Loss 2.7763\n",
            "Adım 1300: Loss 2.6840\n",
            "Adım 1400: Loss 2.5941\n",
            "Adım 1500: Loss 2.5757\n",
            "Adım 1600: Loss 2.4609\n",
            "Adım 1700: Loss 2.4945\n",
            "Adım 1800: Loss 2.4420\n",
            "Adım 1900: Loss 2.4278\n",
            "Adım 2000: Loss 2.3819\n",
            "Adım 2100: Loss 2.3653\n",
            "Adım 2200: Loss 2.3335\n",
            "Adım 2300: Loss 2.3505\n",
            "Adım 2400: Loss 2.2718\n",
            "Adım 2500: Loss 2.2530\n",
            "Adım 2600: Loss 2.2450\n",
            "Adım 2700: Loss 2.1807\n",
            "Adım 2800: Loss 2.1746\n",
            "Adım 2900: Loss 2.1431\n",
            "Adım 3000: Loss 2.1213\n",
            "Adım 3100: Loss 2.0759\n",
            "Adım 3200: Loss 2.0635\n",
            "Adım 3300: Loss 2.0109\n",
            "Adım 3400: Loss 2.0526\n",
            "Adım 3500: Loss 2.0084\n",
            "Adım 3600: Loss 1.9446\n",
            "Adım 3700: Loss 1.9030\n",
            "Adım 3800: Loss 1.9155\n",
            "Adım 3900: Loss 1.9048\n",
            "Adım 4000: Loss 1.8982\n",
            "Adım 4100: Loss 1.8408\n",
            "Adım 4200: Loss 1.8873\n",
            "Adım 4300: Loss 1.8379\n",
            "Adım 4400: Loss 1.8161\n",
            "Adım 4500: Loss 1.7712\n",
            "Adım 4600: Loss 1.7182\n",
            "Adım 4700: Loss 1.7513\n",
            "Adım 4800: Loss 1.7063\n",
            "Adım 4900: Loss 1.6630\n",
            "Adım 5000: Loss 1.6674\n",
            "Adım 5100: Loss 1.6587\n",
            "Adım 5200: Loss 1.6382\n",
            "Adım 5300: Loss 1.5815\n",
            "Adım 5400: Loss 1.5931\n",
            "Adım 5500: Loss 1.6531\n",
            "Adım 5600: Loss 1.5452\n",
            "Adım 5700: Loss 1.5768\n",
            "Adım 5800: Loss 1.5639\n",
            "Adım 5900: Loss 1.5567\n",
            "Eğitim tamamlandı. Final Loss: 1.5243\n",
            "\n",
            "--- Inference (Ödev Formatı) ---\n",
            "O God, O God!\n",
            "\n",
            "ADGELO:\n",
            "Vo, sir, every Ving spake Lortune, and whilesome\n",
            "brocle ere so to spave those courterparrer Away.\n",
            "Grace thou heavent, haar stay, draw thou hast here,\n",
            "And it is his self his vead'st with thy bind.\n",
            "\n",
            "GING EDWARD IV:\n",
            "Ah, sir, I mise by my suit, no, since all he crave:\n",
            "Dobeoble looks be not so.\n",
            "\n",
            "CERIOLANUS:\n",
            "Thy brother-duke of York come,\n",
            "I am the oath of a king, and bother,\n",
            "Wn sorrow, by chear out alsed hands, since malicares!\n",
            "\n",
            "Nurse:\n",
            "Nows for his heart that issure; this heart but breathing\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "# We are downloading and then reading the data.\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).text\n",
        "\n",
        "# The dataset class which is inherited from Dataset Class of PyTorch\n",
        "class CharDataset(Dataset):\n",
        "    #This is the constructer function:\n",
        "    #data: the entire Shakespeare text.\n",
        "    #block_size: the maximum context length the model will see.\n",
        "    #It will not see the whole context at once\n",
        "    #only 128 characters at once for this case\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "        chars = sorted(list(set(data)))\n",
        "        # Here we are finding the unique characters- Vocabulary\n",
        "        #set(data) → takes all unique characters\n",
        "        #list(...) → converts the set to a list\n",
        "        #sorted(...) → sorts alphabetically\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        # Character -> Number and,  Number -> Character conversion dictionaries\n",
        "        #We do this tokenization so the neural network understands the character\n",
        "        #As long as we work with the same data, the character-index mapping always remains the same.\n",
        "        #It's because from chars, the lists comes ordered\n",
        "        self.vocab_size = len(chars)\n",
        "        #for this case our vocab size is 65\n",
        "        #This information is required for the model's embedding table, output layer, etc.\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "        # Returns how many samples we can extract from the dataset\n",
        "        #We extract the block size from the lenght of the data\n",
        "        # so in the last block we don't have empty characters\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Take a piece of text that is block_size + 1 characters long\n",
        "        #We add the plus 1 since the last index is not included in the a:b form\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # Convert characters to numbers\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        # Return the chunk and the shifted version as tensors\n",
        "        # x: The input sequence you will provide to the model (character IDs)(0 to N-1)\n",
        "        # y: The target sequence you want the model to predict (the next character IDs)(1 to N)\n",
        "        #This is the next token prediction concept\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "# Note: Since I was using Google Colab, I reduced the Batch Size to ensure sufficient RAM\n",
        "batch_size = 32      # B: Reduced to manage RAM usage with the larger model\n",
        "block_size = 128     # N: Context window size\n",
        "#Batch size: 32 means the code updates the model after reading 32 pieces of text.\n",
        "#Each of these 32 pieces has 128 characters\n",
        "max_iters = 6000     # Total training steps\n",
        "#This updating process will continue 6000 times\n",
        "#I tried with 3000 but the result wasn't as good\n",
        "#And I had a final loss value as 2.0769\n",
        "learning_rate = 6e-5 # Lower learning rate for stability with deep model\n",
        "#I first used 3e-4 but the loss didn't decrease enough with this value\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 768       #This means the number of embeddings, each character is presented by 768 attributes\n",
        "n_head = 8\n",
        "n_layer = 12       # Number of layers is 12, showing the depth of the code\n",
        "dropout = 0.1      # Standart. This means that we randomly deactivate 10% of neurons during training.\n",
        "                  #This is done to prevent Over-Fitting.\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "class CausalSelfAttn(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "        # Key, Query, Value projection\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        #When attention scores are calculated (after Softmax),\n",
        "        #it randomly drops some connections. This prevents the model from memorizing\n",
        "        #that, this word is 100% connected to that\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Causal Mask (So the model doesn't see the future)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
        "                                     .view(1, 1, block_size, block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "         # Split into heads (B, T, n_head, head_size) -> (B, n_head, T, head_size)\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
        "\n",
        "        # Attention Scors (Scaled Dot-Product)\n",
        "        # Causal Self-Attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1)**0.5))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "\n",
        "        y = att @ v # (B, n_head, T, head_size)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs side by side\n",
        "\n",
        "        return self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  #Multi-Layer Perceptron (Feed-Forward Network)\n",
        "  #The tokens starts processing the information after the Attention procedure\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttn(n_embd, n_head)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.mlp = MLP(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        # out = x + self.MLP(self.LayerNorm_2(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, n_embd)  # Token embeddings\n",
        "        self.wpe = nn.Embedding(block_size, n_embd)  # Position embeddings\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # Final LayerNorm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.wte.weight = self.lm_head.weight\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # tok_emb = WTE(idx)\n",
        "        tok_emb = self.wte(idx)\n",
        "\n",
        "        # pos_emb = WPE(pos)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
        "        pos_emb = self.wpe(pos)\n",
        "\n",
        "        # x = Dropout(tok_emb + pos_emb)\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "\n",
        "        # for Block in Blocks: x = Block(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # x = Final_LayerNorm(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # logits = LM_Head(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "#The training begins here\n",
        "\n",
        "\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# Model Starts\n",
        "model = GPTLanguageModel(vocab_size=train_dataset.get_vocab_size())\n",
        "m = model.to(device)\n",
        "print(f\"Model parametre sayısı: {sum(p.numel() for p in m.parameters())/1e6:.2f} Milyon\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"Eğitim başlıyor (Büyük model olduğu için biraz daha yavaş olabilir)...\")\n",
        "model.train()\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "for iter_num in range(max_iters):\n",
        "    try:\n",
        "        xb, yb = next(data_iter)\n",
        "    except StopIteration:\n",
        "        data_iter = iter(train_loader)\n",
        "        xb, yb = next(data_iter)\n",
        "\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iter_num % 100 == 0: # Sık raporlama\n",
        "        print(f\"Adım {iter_num}: Loss {loss.item():.4f}\")\n",
        "\n",
        "print(f\"Eğitim tamamlandı. Final Loss: {loss.item():.4f}\")\n",
        "\n",
        "#Evaluation and inference happens here\n",
        "\n",
        "print(\"\\n--- Inference ---\")\n",
        "\n",
        "# Tokenizer functions\n",
        "def tokenize(s):\n",
        "    return torch.tensor([train_dataset.stoi[c] for c in s], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "def tokens_to_string(tokens):\n",
        "    return ''.join([train_dataset.itos[i.item()] for i in tokens[0]])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Starter context\n",
        "    context_str = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context_str)\n",
        "\n",
        "    # Generate\n",
        "    y = model.generate(tokenized_context, max_new_tokens=500)\n",
        "\n",
        "    # Decode\n",
        "    completion = tokens_to_string(y)\n",
        "\n",
        "    print(completion)"
      ]
    }
  ]
}