{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Cvru1cgwyP"
      },
      "source": [
        "# **Miniproject 2**\n",
        "## **~Large~ Small Language Model**\n",
        "\n",
        "### **Objective**\n",
        "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
        "\n",
        "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_rT3xwrhieb"
      },
      "source": [
        "### **Dataset**:\n",
        "\n",
        "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
        "\n",
        "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
        "\n",
        "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
        "\n",
        "An interface for the dataset class that takes care of tokenization is provided below.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Emits batches of characters.\n",
        "\n",
        "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, data):\n",
        "\n",
        "        chars = ... # get characters from the input data\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
        "\n",
        "        ...\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __len__(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        # encode every character to an integer\n",
        "        # return the chunk and the shifted version as tensors\n",
        "        pass\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL FIRST AFTER RESTARTING RUNTIME\n",
        "# This sets consistent hyperparameters for the entire notebook\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Download data\n",
        "!wget -q -O input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# FIXED HYPERPARAMETERS - DO NOT CHANGE\n",
        "block_size = 256\n",
        "batch_size = 64\n",
        "max_iters = 5000\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# Dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.block_size = block_size\n",
        "        self.data = data\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = len(chars)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "\n",
        "print(f\"Setup complete!\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Block size: {block_size}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "nDSZI0kO2BGm",
        "outputId": "829afa01-129e-41ea-b9ce-5943f105ca03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete!\n",
            "Device: cuda\n",
            "Block size: 256\n",
            "Vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf138xDHMGZj",
        "outputId": "5dc1227e-2820-44d5-c838-bef791797bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-05 14:10:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-12-05 14:10:53 (28.0 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n",
            "Data downloaded. Total characters: 1115394\n",
            "Dataset is created: 1115394 character, 65 vocabulary.\n",
            "\n",
            "--- Test Başarılı ---\n",
            "Girdi (x) boyutu: torch.Size([128])\n",
            "Hedef (y) boyutu: torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# We are downloading and then reading the data.\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Data downloaded. Total characters: {len(text)}\")\n",
        "#len(text) calculates the number of characters including space and punctuation marks.\n",
        "#This information is important since our model is a character level model\n",
        "\n",
        "\n",
        "# The dataset class which is inherited from Dataset Class of PyTorch\n",
        "class CharDataset(Dataset):\n",
        "    #This is the constructer function:\n",
        "    #data: the entire Shakespeare text.\n",
        "    #block_size: the maximum context length the model will see.\n",
        "    #It will not see the whole context at once\n",
        "    #only 128 characters at once for this case\n",
        "    def __init__(self, data, block_size):\n",
        "        self.block_size = block_size\n",
        "        self.data = data\n",
        "\n",
        "        # Here we are finding the unique characters- Vocabulary\n",
        "        #set(data) → takes all unique characters\n",
        "        #list(...) → converts the set to a list\n",
        "        #sorted(...) → sorts alphabetically\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print(f'Dataset is created: {data_size} character, {vocab_size} vocabulary.')\n",
        "        #for this case our vocab size is 65\n",
        "\n",
        "        # Character -> Number and,  Number -> Character conversion dictionaries\n",
        "        #We do this tokenization so the neural network understands the character\n",
        "        #As long as we work with the same data, the character-index mapping always remains the same.\n",
        "        #It's because from chars the lists comes ordered\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.vocab_size = vocab_size\n",
        "        #This information is required for the model's embedding table, output layer, etc.\n",
        "\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns how many samples we can extract from the dataset\n",
        "        #We extract the block size from the lenght of the data\n",
        "        # so in the last block we don't have empty characters\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Take a piece of text that is block_size + 1 characters long\n",
        "        #We add the plus 1 since the last index is not included in the a:b form\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "\n",
        "        # Convert characters to numbers\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "\n",
        "        # Convert to PyTorch Tensor\n",
        "        #x: The input sequence you will provide to the model (character IDs)\n",
        "        #y: The target sequence you want the model to predict (the next character IDs)\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# 3. Test Kısmı\n",
        "block_size = 128 # Modelin hafızası (Context Window)\n",
        "train_dataset = CharDataset(text, block_size)\n",
        "\n",
        "# İlk örneği çekip bakalım, çalışıyor mu?\n",
        "x, y = train_dataset[0]\n",
        "print(\"\\n--- Test Başarılı ---\")\n",
        "print(\"Girdi (x) boyutu:\", x.shape)\n",
        "print(\"Hedef (y) boyutu:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "jVv9AEarETdw",
        "outputId": "8b7f5264-2c9a-4853-9867-eaa1459c9941"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2545953230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 1. HAZIRLIK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Ensure consistent hyperparameters\n",
        "block_size = 256\n",
        "batch_size = 64\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 1. HAZIRLIK\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 2. EĞİTİM DÖNGÜSÜ BAŞLIYOR\n",
        "print(f\"Eğitim başlıyor... Hedef: {max_iters} adım.\")\n",
        "print(f\"Cihaz: {device} (Eğer 'cpu' ise biraz yavaş olabilir, sabırlı ol)\")\n",
        "\n",
        "model.train()\n",
        "\n",
        "for step, (xb, yb) in enumerate(train_loader):\n",
        "    # Veriyi Cihaza (GPU veya CPU) gönder\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    # A. TAHMİN ET (Forward Pass)\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # B. HATAYI ÖLÇ VE GERİ GÖNDER (Backward Pass)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Durum Raporu (Her 100 adımda bir yazdır) - INSIDE THE LOOP!\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Adım {step}: Hata Puanı (Loss) = {loss.item():.4f}\")\n",
        "\n",
        "    # Belirlenen adım sayısına ulaşınca dur - INSIDE THE LOOP!\n",
        "    if step >= max_iters:\n",
        "        break\n",
        "\n",
        "print(\"Eğitim tamamlandı! Model artık Shakespeare gibi konuşabilir.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_bNWdWaRzRj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hiperparametreler (Modelin ayarları)\n",
        "n_embd = 32       # Her harfin vektör uzunluğu (Embedding boyutu)\n",
        "head_size = 16    # Bu \"Kafa\"nın ilgileneceği parça boyutu\n",
        "dropout = 0.1     # Ezberlemeyi önlemek için unutma oranı\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" Tek bir Self-Attention Kafası \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # 1. Key, Query ve Value katmanları (Dedektif araçları)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # 2. Maskeleme Matrisi (Geleceği görmeyi engellemek için)\n",
        "        # 'tril' : Triangular Lower Matrix (Alt Üçgen Matris)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x'in boyutu: (Batch_Size, Time_Step, Channels) -> (B,T,C)\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        # 3. Sorgu (Query) ve Anahtar (Key) üretimi\n",
        "        k = self.key(x)   # (B,T,16)\n",
        "        q = self.query(x) # (B,T,16)\n",
        "\n",
        "        # 4. Dikkat Skorlarını Hesapla (İlişki kurma anı)\n",
        "        # (B, T, 16) @ (B, 16, T) -> (B, T, T) matrisi oluşur\n",
        "        # C**-0.5 ile çarpma işlemi sayıların çok büyümesini engeller (Normalization)\n",
        "        wei = q @ k.transpose(-2, -1) * (C**-0.5)\n",
        "\n",
        "        # 5. Maskeleme: Gelecekteki harfleri kapat (-sonsuz yap)\n",
        "        # Böylece model sadece geçmişe bakabilir\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # 6. Olasılığa çevir (Softmax)\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # 7. Değerleri (Value) bu olasılıklarla çarpıp topla\n",
        "        v = self.value(x) # (B,T,16)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, 16) -> (B, T, 16)\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"Head (Dikkat Kafası) sınıfı tanımlandı.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37iCQZCrAgUh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. THE TEAM (Ekip Kurulumu)\n",
        "        # Head sınıfından 'num_heads' kadar oluşturup bir listeye koyuyoruz.\n",
        "        # nn.ModuleList kullanmak zorundayız, yoksa PyTorch bu katmanları tanımaz ve eğitmez.\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "        # 2. PROJECTION (Birleştirme ve Karıştırma)\n",
        "        # Tüm kafalardan gelen veriyi birleştirdikten sonra son bir kez işliyoruz.\n",
        "        # Bu, farklı kafaların bulduğu bilgilerin birbirine karışmasını sağlar.\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # 3. DROPOUT (Unutma)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 4. RUN PARALLEL (Paralel Çalıştırma)\n",
        "        # x verisini her bir kafa (h) için ayrı ayrı çalıştır.\n",
        "        # Sonuçları bir liste haline getir.\n",
        "        out = [h(x) for h in self.heads]\n",
        "\n",
        "        # 5. CONCATENATE (Yapıştırma)\n",
        "        # Elimizdeki listeyi 'channel' boyutunda (son boyut) yan yana yapıştır.\n",
        "        # Örneğin: 4 tane (B, T, 16) matrisini yapıştırırsak -> (B, T, 64) olur.\n",
        "        out = torch.cat(out, dim=-1)\n",
        "\n",
        "        # 6. FINAL PROCESSING (Son İşlem)\n",
        "        # Birleşmiş veriyi projeksiyon katmanından geçir ve dropout uygula.\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"MultiHeadAttention sınıfı tanımlandı.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzxshlxCAwz7"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        # nn.Sequential: A container that runs layers in order (Sırayla çalıştırır)\n",
        "        self.net = nn.Sequential(\n",
        "            # 1. EXPANSION (Genişleme)\n",
        "            # We expand the dimension by 4 to give the model more \"thinking space\".\n",
        "            # Input: n_embd (32) -> Output: 4 * n_embd (128)\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "\n",
        "            # 2. ACTIVATION (Aktivasyon)\n",
        "            # ReLU (Rectified Linear Unit) turns negative numbers to zero.\n",
        "            # It allows the model to learn complex, non-linear patterns.\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # 3. PROJECTION (Eski Boyuta Dönüş)\n",
        "            # We compress the processed information back to the original size.\n",
        "            # Input: 128 -> Output: 32\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "\n",
        "            # 4. DROPOUT (Unutma)\n",
        "            # Randomly zeroes some elements to prevent overfitting (ezber).\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the sequential layers\n",
        "        return self.net(x)\n",
        "\n",
        "print(\"FeedForward sınıfı tanımlandı.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaY8oKVBBMd"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension (Vektör boyutu)\n",
        "        # n_head: the number of heads we'd like (Kaç kafa çalışacak)\n",
        "        super().__init__()\n",
        "\n",
        "        # Calculate size of each head (Her kafanın boyutu)\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        # 1. COMMUNICATION (İletişim - Attention)\n",
        "        # The \"Social\" layer where tokens talk to each other.\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "\n",
        "        # 2. COMPUTATION (Hesaplama - FeedForward)\n",
        "        # The \"Personal\" layer where tokens think about what they heard.\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "        # 3. NORMALIZATION (Standartlaştırma)\n",
        "        # Keeps the numbers stable so training doesn't crash.\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 4. RESIDUAL CONNECTION + ATTENTION\n",
        "        # x + ... means \"Keep the old information, add the new one on top\".\n",
        "        # We normalize (ln1) BEFORE attention (Pre-Norm architecture).\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # 5. RESIDUAL CONNECTION + FEEDFORWARD\n",
        "        # Again, we normalize (ln2) BEFORE computation.\n",
        "        # This structure is crucial for deep networks (allows gradients to flow).\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"Block (Transformer Bloğu) sınıfı tanımlandı.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQGlzX62BUAW"
      },
      "outputs": [],
      "source": [
        "# 1. Önce eksik olan parçayı tanımlayalım\n",
        "# Verisetinden alfabe büyüklüğünü öğreniyoruz\n",
        "vocab_size = train_dataset.vocab_size\n",
        "print(f\"Vocab size tanımlandı: {vocab_size}\")\n",
        "\n",
        "# 2. Şimdi Modeli tanımlıyoruz\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # token embedding table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # position embedding table\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # blocks (Apartman katları)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        # language model head\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop context if it becomes too large\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# Modeli oluştur ve Cihaza (GPU/CPU) gönder\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# Parametre sayısını yazdır\n",
        "print(f\"Model başarıyla oluşturuldu! {sum(p.numel() for p in m.parameters())/1e6:.2f} Milyon parametre.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. HAZIRLIK\n",
        "# Optimizer: Modelin hatalarından ders çıkarmasını sağlayan \"Öğretmen\" (AdamW algoritması)\n",
        "# lr (learning rate): Öğrenme hızı. Çok yüksekse ezberler, çok düşükse öğrenmez.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# DataLoader: Veriyi \"batch_size\" kadar paketleyip modele sunan \"Garson\"\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 2. EĞİTİM DÖNGÜSÜ BAŞLIYOR\n",
        "print(f\"Eğitim başlıyor... Hedef: {max_iters} adım.\")\n",
        "print(f\"Cihaz: {device} (Eğer 'cpu' ise biraz yavaş olabilir, sabırlı ol)\")\n",
        "\n",
        "model.train()  # Modeli \"Öğrenci Moduna\" al\n",
        "\n",
        "for step, (xb, yb) in enumerate(train_loader):\n",
        "    # Veriyi Cihaza (GPU veya CPU) gönder\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    # A. TAHMİN ET (Forward Pass)\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # B. HATAYI ÖLÇ VE GERİ GÖNDER (Backward Pass)\n",
        "    optimizer.zero_grad(set_to_none=True)  # Eski hataları temizle\n",
        "    loss.backward()  # Hatayı geriye doğru yay\n",
        "    optimizer.step()  # Ağırlıkları güncelle (Öğrenme anı!)\n",
        "\n",
        "    # Durum Raporu (Her 100 adımda bir yazdır)\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Adım {step}: Hata Puanı (Loss) = {loss.item():.4f}\")\n",
        "\n",
        "    # Belirlenen adım sayısına ulaşınca dur\n",
        "    if step >= max_iters:\n",
        "        break\n",
        "\n",
        "print(\"Eğitim tamamlandı! Model artık Shakespeare gibi konuşabilir.\")"
      ],
      "metadata": {
        "id": "icpVCwtOolwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7OAXGRhf_V"
      },
      "source": [
        "### **Requirements**\n",
        "\n",
        "#### **Architecture**\n",
        "\n",
        "Implement the Transformer's decoder-only structure.\n",
        "This includes\n",
        "\n",
        "* input token embeddings\n",
        "* the causal multi-head self-attention mechanism\n",
        "* feed-forward neural networks\n",
        "* positional encodings, residual connections, layer normalizations.\n",
        "\n",
        "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
        "\n",
        "The `forward` method for the entire model has the following form:\n",
        "\n",
        "```\n",
        "tok_emb = WTE(idx) # token embeddings\n",
        "pos_emb = WPE(pos) # position embeddings\n",
        "x = Dropout(tok_emb + pos_emb)\n",
        "for Block in Blocks:\n",
        "    x = Block(x)\n",
        "x = Final_LayerNorm(x)\n",
        "logits = LM_Head(x)\n",
        "```\n",
        "\n",
        "The `forward` method for the transformer block has the following form:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
        "out = x + self.MLP(self.LayerNorm_2(x))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **Training**\n",
        "\n",
        "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
        "\n",
        "Preprocess the dataset to a character-level representation.\n",
        "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
        "Implement causal masking for the self-attention mechanism.\n",
        "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
        "\n",
        "**Optional**:\n",
        "\n",
        "* Implement a learning rate decay strategy\n",
        "* Implement gradient clipping\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **Evaluation and Inference**\n",
        "\n",
        "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
        "\n",
        "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
        "\n",
        "The high-level pseudocode for generation is:\n",
        "\n",
        "```python\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    tokenized_context = tokenize(context)\n",
        "    # the model should implement a method to generate tokens given a prompt\n",
        "    y = model.generate(tokenized, ...)\n",
        "    completion = tokens_to_string(y)\n",
        "```\n",
        "\n",
        "**Optional**:\n",
        "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t88Dcn8JZ8M"
      },
      "source": [
        "### **Example Outputs**\n",
        "\n",
        "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "O God, O God! neither? unto the base very ears,\n",
        "As damned with it.\n",
        "\n",
        "DUKE OF YORK:\n",
        "Away! Once more, one word.\n",
        "\n",
        "RICHARD:\n",
        "Clove, dear so; and therein my son will be\n",
        "false of woe: if ye seems to be the mother\n",
        "Of gracious order this time when R going kinsperse eyes,\n",
        "What dost bewreck her fairer drying tears.\n",
        "\n",
        "NORTHUMBERLAND:\n",
        "Have you forgot the Duke of Norfolk, get him to\n",
        "again; and and agilic: there is my spirit\n",
        "So maly did must such a marble perfection.\n",
        "\n",
        "ELBOW:\n",
        "Come, bring them with oaths, and so deliver\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0SY7CGAhnkp"
      },
      "source": [
        "### Resources:\n",
        "\n",
        "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "\n",
        "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZdSRWPmgt-H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}